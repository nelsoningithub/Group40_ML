{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu118\n",
      "0.20.1+cu118\n",
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import torch\n",
    "import torchvision  # Add this line\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "# Debugging information\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)  \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying training files...\n",
      "Copying testing files...\n",
      "Copying testing files2...\n",
      "Dataset split complete.\n",
      "Training files: 6000\n",
      "Testing files: 4000\n",
      "Testing files2: 2477\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = \"./dataset\"\n",
    "NO_WATERMARK_DIR = os.path.join(BASE_DIR, \"no_watermark\")\n",
    "WATERMARKED_DIR = os.path.join(BASE_DIR, \"watermarked\")\n",
    "MASKS_DIR = os.path.join(BASE_DIR, \"masks\")\n",
    "\n",
    "OUTPUT_DIR = \"./dataset_split\"\n",
    "TRAIN_DIR = os.path.join(OUTPUT_DIR, \"train1\")\n",
    "TEST_DIR = os.path.join(OUTPUT_DIR, \"test1\")\n",
    "TEST2_DIR = os.path.join(OUTPUT_DIR, \"test2\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(TRAIN_DIR, \"no_watermark\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TRAIN_DIR, \"watermarked\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TRAIN_DIR, \"masks\"), exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(TEST_DIR, \"no_watermark\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TEST_DIR, \"watermarked\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TEST_DIR, \"masks\"), exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(TEST2_DIR, \"no_watermark\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TEST2_DIR, \"watermarked\"), exist_ok=True)\n",
    "\n",
    "# File names\n",
    "file_names = sorted(os.listdir(NO_WATERMARK_DIR))\n",
    "total_files = len(file_names)\n",
    "\n",
    "# Split sizes\n",
    "TRAIN_SPLIT = 6000  # 6000 for training\n",
    "TEST_SPLIT = 10000  # Remaining 50% for testing\n",
    "\n",
    "train_files = file_names[:TRAIN_SPLIT]\n",
    "test_files = file_names[TRAIN_SPLIT:TEST_SPLIT]\n",
    "test2_files = file_names[TEST_SPLIT:]\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(file_list, src_dir, dest_dir):\n",
    "    for file_name in file_list:\n",
    "        src_path = os.path.join(src_dir, file_name)\n",
    "        dest_path = os.path.join(dest_dir, file_name)\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Copy training files\n",
    "print(\"Copying training files...\")\n",
    "copy_files(train_files, NO_WATERMARK_DIR, os.path.join(TRAIN_DIR, \"no_watermark\"))\n",
    "copy_files(train_files, WATERMARKED_DIR, os.path.join(TRAIN_DIR, \"watermarked\"))\n",
    "copy_files(train_files, MASKS_DIR, os.path.join(TRAIN_DIR, \"masks\"))\n",
    "\n",
    "# Copy testing files (only watermarked and masks)\n",
    "print(\"Copying testing files...\")\n",
    "copy_files(test_files, NO_WATERMARK_DIR, os.path.join(TEST_DIR, \"no_watermark\"))\n",
    "copy_files(test_files, WATERMARKED_DIR, os.path.join(TEST_DIR, \"watermarked\"))\n",
    "copy_files(test_files, MASKS_DIR, os.path.join(TEST_DIR, \"masks\"))\n",
    "\n",
    "print(\"Copying testing files2...\")\n",
    "copy_files(test2_files, NO_WATERMARK_DIR, os.path.join(TEST2_DIR, \"no_watermark\"))\n",
    "copy_files(test2_files, WATERMARKED_DIR, os.path.join(TEST2_DIR, \"watermarked\"))\n",
    "\n",
    "# Summary\n",
    "print(f\"Dataset split complete.\")\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Testing files: {len(test_files)}\")\n",
    "print(f\"Testing files2: {len(test2_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 6000/6000 [05:44<00:00, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 6000/6000 [03:09<00:00, 31.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 6000/6000 [03:51<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 6000/6000 [05:16<00:00, 18.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 6000/6000 [04:48<00:00, 20.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.0322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 6000/6000 [04:24<00:00, 22.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 6000/6000 [04:23<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 6000/6000 [04:20<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 6000/6000 [04:16<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 6000/6000 [04:36<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 6000/6000 [03:51<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 6000/6000 [04:10<00:00, 23.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 6000/6000 [03:46<00:00, 26.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 6000/6000 [03:23<00:00, 29.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 6000/6000 [03:32<00:00, 28.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 6000/6000 [03:29<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 6000/6000 [03:30<00:00, 28.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 6000/6000 [03:28<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 6000/6000 [03:25<00:00, 29.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 6000/6000 [03:27<00:00, 28.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.0214\n",
      "Feature Extractor saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "WINDOW_SIZE = 64\n",
    "STRIDE = 32\n",
    "\n",
    "# Dataset paths\n",
    "TRAIN_DIR = \"./dataset_split/train1\"\n",
    "TEST_DIR = \"./dataset_split/test1\"\n",
    "MASK_OUTPUT_DIR = \"./test_mask_results\"\n",
    "os.makedirs(MASK_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Data transformations (no resizing to retain original sizes)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert to tensor while retaining original size\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class WatermarkSlidingWindowDataset(Dataset):\n",
    "    def __init__(self, watermark_dir, mask_dir, transform=None):\n",
    "        self.watermark_dir = watermark_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = sorted(os.listdir(watermark_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        watermark_path = os.path.join(self.watermark_dir, self.images[idx])\n",
    "        watermark_img = Image.open(watermark_path).convert(\"RGB\")\n",
    "        \n",
    "        mask_img = None  # Default to None in case masks are not used\n",
    "        if self.mask_dir:\n",
    "            mask_path = os.path.join(self.mask_dir, self.images[idx])\n",
    "            mask_img = Image.open(mask_path).convert(\"L\")  # Mask is single-channel\n",
    "\n",
    "        original_size = watermark_img.size  # Save original image size (width, height)\n",
    "        filename = self.images[idx]  # Save the filename for reference\n",
    "\n",
    "        if self.transform:\n",
    "            watermark_img = self.transform(watermark_img)\n",
    "            if mask_img is not None:\n",
    "                mask_img = self.transform(mask_img)\n",
    "\n",
    "        return watermark_img, mask_img, original_size, filename\n",
    "\n",
    "\n",
    "# Custom collate_fn to handle varying image sizes\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "# Load training and testing datasets\n",
    "train_dataset = WatermarkSlidingWindowDataset(\n",
    "    watermark_dir=os.path.join(TRAIN_DIR, \"watermarked\"),\n",
    "    mask_dir=os.path.join(TRAIN_DIR, \"masks\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = WatermarkSlidingWindowDataset(\n",
    "    watermark_dir=os.path.join(TEST_DIR, \"watermarked\"),\n",
    "    mask_dir=os.path.join(TEST_DIR, \"masks\"),\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class UNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNetFeatureExtractor, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling layers\n",
    "        self.encoder1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Middle layer\n",
    "        self.middle = self.conv_block(512, 1024, is_middle=True)\n",
    "        \n",
    "        # Decoder: Upsampling layers\n",
    "        self.decoder4 = self.upconv_block(1024, 512)\n",
    "        self.decoder3 = self.upconv_block(512, 256)\n",
    "        self.decoder2 = self.upconv_block(256, 128)\n",
    "        self.decoder1 = self.upconv_block(128, 64)\n",
    "\n",
    "        # Layers to adjust channel dimensions after skip connections\n",
    "        self.conv4 = nn.Conv2d(1024, 512, kernel_size=1)  # Adjust channels\n",
    "        self.conv3 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv1 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Outputs a mask with values in [0, 1]\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, is_middle=False):\n",
    "        \"\"\"Helper function to create convolutional blocks, mainly for the encoder and middle layers.\"\"\"\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if not is_middle:\n",
    "            layers.append(nn.MaxPool2d(2))  # Downsample\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Upsampling block in the decoder, using transpose convolution.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding phase\n",
    "        enc1 = self.encoder1(x)  # Output: 64 channels\n",
    "        enc2 = self.encoder2(enc1)  # Output: 128 channels\n",
    "        enc3 = self.encoder3(enc2)  # Output: 256 channels\n",
    "        enc4 = self.encoder4(enc3)  # Output: 512 channels\n",
    "\n",
    "        # Middle layer\n",
    "        middle = self.middle(enc4)  # Output: 1024 channels\n",
    "        \n",
    "        # Decoding phase with skip connections\n",
    "        dec4 = self.decoder4(middle)  # Output: 512 channels\n",
    "        dec4 = F.interpolate(dec4, size=enc4.shape[2:], mode='bilinear', align_corners=False)  # Match size\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)  # Skip connection\n",
    "        dec4 = self.conv4(dec4)  # Adjust channels\n",
    "\n",
    "        dec3 = self.decoder3(dec4)  # Output: 256 channels\n",
    "        dec3 = F.interpolate(dec3, size=enc3.shape[2:], mode='bilinear', align_corners=False)  # Match size\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)  # Skip connection\n",
    "        dec3 = self.conv3(dec3)  # Adjust channels\n",
    "\n",
    "        dec2 = self.decoder2(dec3)  # Output: 128 channels\n",
    "        dec2 = F.interpolate(dec2, size=enc2.shape[2:], mode='bilinear', align_corners=False)  # Match size\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)  # Skip connection\n",
    "        dec2 = self.conv2(dec2)  # Adjust channels\n",
    "\n",
    "        dec1 = self.decoder1(dec2)  # Output: 64 channels\n",
    "        dec1 = F.interpolate(dec1, size=enc1.shape[2:], mode='bilinear', align_corners=False)  # Match size\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)  # Skip connection\n",
    "        dec1 = self.conv1(dec1)  # Adjust channels\n",
    "\n",
    "        # Final output\n",
    "        out = self.final_conv(dec1)  # Output: 1 channel\n",
    "        out = self.sigmoid(out)  # Mask output with values in [0, 1]\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = UNetFeatureExtractor(in_channels=3, out_channels=1).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        watermarked_imgs, masks, original_sizes, filenames = batch[0]\n",
    "        watermarked_imgs = watermarked_imgs.to(DEVICE).unsqueeze(0)  # Add batch dimension\n",
    "        masks = masks.to(DEVICE).unsqueeze(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        mask_pred = model(watermarked_imgs)\n",
    "        # Resize predicted mask to target size\n",
    "        _, _, target_h, target_w = masks.shape\n",
    "        mask_pred = torch.nn.functional.interpolate(mask_pred, size=(target_h, target_w), mode='bilinear', align_corners=True)\n",
    "        loss = criterion(mask_pred, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Save the model\n",
    "FEATURE_EXTRACTOR_PATH = \"feature_extractor.pth\"\n",
    "torch.save(model.state_dict(), FEATURE_EXTRACTOR_PATH)\n",
    "print(\"Feature Extractor saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9192\\3023910116.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [02:07<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All masks generated and saved.\n"
     ]
    }
   ],
   "source": [
    "def test_feature_extractor(test_loader, model_path, mask_path):\n",
    "    print(\"Testing feature extractor...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader)):\n",
    "            # Extract watermarked image, mask, original size, and filename\n",
    "            watermarked_imgs, masks, original_sizes, filenames = batch[0]\n",
    "\n",
    "            # Debug: Print the filename being processed\n",
    "            # print(f\"Processing image: {filenames}\")\n",
    "\n",
    "            watermarked_imgs = watermarked_imgs.to(DEVICE).unsqueeze(0)\n",
    "            mask_pred = model(watermarked_imgs)\n",
    "\n",
    "            # Resize predicted mask to original size\n",
    "            original_size = original_sizes  # (width, height)\n",
    "            filename = filenames\n",
    "\n",
    "            # Extract base name and ensure valid file extension\n",
    "            base_name, ext = os.path.splitext(filename)\n",
    "            if ext.lower() not in [\".jpg\", \".jpeg\", \".png\"]:  # Handle invalid extensions\n",
    "                ext = \".png\"  # Default to PNG if extension is missing or invalid\n",
    "\n",
    "            mask_pred_img = transforms.ToPILImage()(mask_pred.squeeze(0).cpu())\n",
    "            mask_pred_resized = mask_pred_img.resize(original_size, Image.BILINEAR)\n",
    "\n",
    "            # Save predicted mask\n",
    "            output_file = os.path.join(mask_path, f\"predicted_mask_{base_name}{ext}\")\n",
    "            mask_pred_resized.save(output_file)\n",
    "\n",
    "            # print(f\"Saved mask: {output_file}\")\n",
    "\n",
    "    print(\"All masks generated and saved.\")\n",
    "\n",
    "# Test the feature extractor\n",
    "test_feature_extractor(test_loader, FEATURE_EXTRACTOR_PATH, MASK_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9192\\3023910116.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2477/2477 [00:53<00:00, 46.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All masks generated and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create the mask for test2\n",
    "\n",
    "# define paramete\n",
    "PART2_TEST_DIR = \"./dataset_split/test2\" # later will redefine\n",
    "PART2_TEST_MASK_DIR = \"./test_model_result/mask\" # later will redefine\n",
    "\n",
    "part2_test_dataset = WatermarkSlidingWindowDataset(\n",
    "    watermark_dir=os.path.join(PART2_TEST_DIR, \"watermarked\"),\n",
    "    mask_dir='',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "part2_test_loader = DataLoader(part2_test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# predict mask\n",
    "test_feature_extractor(part2_test_loader, FEATURE_EXTRACTOR_PATH, PART2_TEST_MASK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training for watermark removal model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   3%|▎         | 111/4000 [00:05<02:56, 22.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 206\u001b[0m\n\u001b[0;32m    204\u001b[0m part2_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    205\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(part2_train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPART2_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    207\u001b[0m \n\u001b[0;32m    208\u001b[0m     \u001b[38;5;66;03m# Unpack the batch\u001b[39;00m\n\u001b[0;32m    209\u001b[0m     watermarked_imgs, masks, clean_imgs, filenames \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Move images and masks to the device (GPU or CPU)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 60\u001b[0m, in \u001b[0;36mWatermarkRemovalDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask not found for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwatermarked_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m clean_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_files\u001b[38;5;241m.\u001b[39mget(watermarked_filename) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_files \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_dir \u001b[38;5;129;01mand\u001b[39;00m clean_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(clean_path):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean image not found for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwatermarked_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m watermarked_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(watermarked_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os  # Import the os module\n",
    "import torch\n",
    "import shutil\n",
    "from torch.utils.data import DataLoader, Dataset  # Add Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Set device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Hyperparameters\n",
    "PART2_EPOCHS = 5 # 20\n",
    "PART2_LEARNING_RATE = 0.0001\n",
    "PART2_WINDOW_SIZE = 64\n",
    "PART2_STRIDE = 32\n",
    "\n",
    "# Paths\n",
    "PART2_TRAIN_DIR = \"./dataset_split/test1\"\n",
    "PART2_TEST_DIR = \"./dataset_split/test2\"\n",
    "PART2_TRAIN_MASK_DIR = \"./test_mask_results\"\n",
    "PART2_TEST_MASK_DIR = \"./test_model_result/mask\"\n",
    "PART2_OUTPUT_DIR = \"./test_model_result/result\"\n",
    "\n",
    "# Create output directory for test results\n",
    "os.makedirs(PART2_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Data transformations (no resizing to retain original sizes)\n",
    "transform = transforms.Compose([transforms.ToTensor()])  # Convert to tensor while retaining original size\n",
    "\n",
    "# Custom Dataset for second part of training\n",
    "class WatermarkRemovalDataset(Dataset):\n",
    "    def __init__(self, watermarked_dir, clean_dir=None, mask_dir=None, transform=None):\n",
    "        self.watermarked_dir = watermarked_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.watermarked_files = sorted(os.listdir(watermarked_dir))\n",
    "        self.mask_files = {f: os.path.join(mask_dir, f) for f in os.listdir(mask_dir)} if mask_dir else None\n",
    "        self.clean_files = {f: os.path.join(clean_dir, f) for f in os.listdir(clean_dir)} if clean_dir else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.watermarked_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        watermarked_filename = self.watermarked_files[idx]\n",
    "        watermarked_path = os.path.join(self.watermarked_dir, watermarked_filename)\n",
    "\n",
    "        mask_path = self.mask_files.get(f'predicted_mask_{watermarked_filename}') if self.mask_files else None\n",
    "        if mask_path and not os.path.exists(mask_path):\n",
    "            raise FileNotFoundError(f\"Mask not found for file: {watermarked_filename}\")\n",
    "\n",
    "        clean_path = self.clean_files.get(watermarked_filename) if self.clean_files else None\n",
    "        if self.clean_dir and clean_path and not os.path.exists(clean_path):\n",
    "            raise FileNotFoundError(f\"Clean image not found for file: {watermarked_filename}\")\n",
    "\n",
    "        watermarked_img = Image.open(watermarked_path).convert(\"RGB\")\n",
    "        mask_img = Image.open(mask_path).convert(\"L\") if mask_path else None\n",
    "        clean_img = Image.open(clean_path).convert(\"RGB\") if clean_path else None\n",
    "\n",
    "        if mask_img:\n",
    "            mask_img = mask_img.resize(watermarked_img.size, Image.NEAREST)\n",
    "        if clean_img:\n",
    "            clean_img = clean_img.resize(watermarked_img.size, Image.NEAREST)\n",
    "\n",
    "        if self.transform:\n",
    "            watermarked_img = self.transform(watermarked_img)\n",
    "            mask_img = self.transform(mask_img) if mask_img else None\n",
    "            clean_img = self.transform(clean_img) if clean_img else None\n",
    "\n",
    "        return watermarked_img, mask_img, clean_img, watermarked_filename\n",
    "\n",
    "# Custom collate_fn to handle varying image sizes\n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "# Load datasets\n",
    "part2_train_dataset = WatermarkRemovalDataset(\n",
    "    watermarked_dir=os.path.join(PART2_TRAIN_DIR, \"watermarked\"),\n",
    "    clean_dir=os.path.join(PART2_TRAIN_DIR, \"no_watermark\"),\n",
    "    mask_dir=PART2_TRAIN_MASK_DIR,\n",
    "    transform=transform\n",
    ")\n",
    "part2_test_dataset = WatermarkRemovalDataset(\n",
    "    watermarked_dir=os.path.join(PART2_TEST_DIR, \"watermarked\"),\n",
    "    clean_dir=os.path.join(PART2_TRAIN_DIR, \"no_watermark\"),\n",
    "    mask_dir=PART2_TEST_MASK_DIR,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "part2_train_loader = DataLoader(part2_train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "part2_test_loader = DataLoader(part2_test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Simple UNet-like architecture for second part\n",
    "class UNetForWatermarkRemoval(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(UNetForWatermarkRemoval, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling layers\n",
    "        self.encoder1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Middle layer\n",
    "        self.middle = self.conv_block(512, 1024, is_middle=True)\n",
    "        \n",
    "        # Decoder: Upsampling layers\n",
    "        self.decoder4 = self.upconv_block(1024, 512)\n",
    "        self.decoder3 = self.upconv_block(512, 256)\n",
    "        self.decoder2 = self.upconv_block(256, 128)\n",
    "        self.decoder1 = self.upconv_block(128, 64)\n",
    "\n",
    "        # Layers to adjust channel dimensions after skip connections\n",
    "        self.conv4 = nn.Conv2d(1024, 512, kernel_size=1)  # Adjust channels\n",
    "        self.conv3 = nn.Conv2d(512, 256, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=1)\n",
    "        self.conv1 = nn.Conv2d(128, 64, kernel_size=1)\n",
    "        \n",
    "        # Final output layer (3 channels for RGB output)\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels, is_middle=False):\n",
    "        \"\"\"Helper function to create convolutional blocks, mainly for the encoder and middle layers.\"\"\"\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if not is_middle:\n",
    "            layers.append(nn.MaxPool2d(2))  # Downsample\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def upconv_block(self, in_channels, out_channels):\n",
    "        \"\"\"Upsampling block in the decoder, using transpose convolution.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Resize mask to match the input dimensions\n",
    "        mask = F.interpolate(mask, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Combine watermarked image and mask\n",
    "        x = torch.cat([x, mask], dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "        # Encoding phase\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(enc1)\n",
    "        enc3 = self.encoder3(enc2)\n",
    "        enc4 = self.encoder4(enc3)\n",
    "\n",
    "        # Middle layer\n",
    "        middle = self.middle(enc4)\n",
    "\n",
    "        # Decoding phase with skip connections\n",
    "        dec4 = self.decoder4(middle)\n",
    "        dec4 = F.interpolate(dec4, size=enc4.shape[2:], mode='bilinear', align_corners=False)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)\n",
    "        dec4 = self.conv4(dec4)\n",
    "\n",
    "        dec3 = self.decoder3(dec4)\n",
    "        dec3 = F.interpolate(dec3, size=enc3.shape[2:], mode='bilinear', align_corners=False)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.conv3(dec3)\n",
    "\n",
    "        dec2 = self.decoder2(dec3)\n",
    "        dec2 = F.interpolate(dec2, size=enc2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.conv2(dec2)\n",
    "\n",
    "        dec1 = self.decoder1(dec2)\n",
    "        dec1 = F.interpolate(dec1, size=enc1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.conv1(dec1)\n",
    "\n",
    "        # Final output\n",
    "        out = self.final_conv(dec1)\n",
    "\n",
    "        # Resize output to match input dimensions\n",
    "        out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize VGG19 model for perceptual loss\n",
    "vgg19 = models.vgg19(pretrained=True).features.eval().to(DEVICE)\n",
    "\n",
    "# Define the perceptual loss using VGG19 features\n",
    "def perceptual_loss(x, y):\n",
    "    \"\"\"Compute perceptual loss using a pretrained VGG model.\"\"\"\n",
    "    x_vgg = vgg19(x)\n",
    "    y_vgg = vgg19(y)\n",
    "    return F.mse_loss(x_vgg, y_vgg)\n",
    "\n",
    "# Define combined loss function (MSE + Perceptual Loss)\n",
    "def combined_loss(output, target, alpha=0.7, beta=0.3):\n",
    "    \"\"\"\n",
    "    Combines MSE loss and perceptual loss.\n",
    "    alpha: Weight for MSE loss\n",
    "    beta: Weight for perceptual loss\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(output, target)  # Mean Squared Error Loss\n",
    "    p_loss = perceptual_loss(output, target)  # Perceptual Loss\n",
    "    return alpha * mse + beta * p_loss  # Weighted combination\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "part2_model = UNetForWatermarkRemoval(in_channels=4, out_channels=3).to(DEVICE)  # In: 3 for image + 1 for mask\n",
    "part2_optimizer = optim.Adam(part2_model.parameters(), lr=0.0001)\n",
    "part2_criterion = combined_loss  # Use combined loss function\n",
    "\n",
    "# Training loop for the image generation model (watermark removal)\n",
    "print(\"Starting training for watermark removal model...\")\n",
    "\n",
    "# Assuming `EPOCHS` is defined, you can replace with actual number of epochs\n",
    "for epoch in range(PART2_EPOCHS):\n",
    "    part2_model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(part2_train_loader, desc=f\"Epoch {epoch+1}/{PART2_EPOCHS}\"):\n",
    "\n",
    "        # Unpack the batch\n",
    "        watermarked_imgs, masks, clean_imgs, filenames = batch[0]\n",
    "\n",
    "        # Move images and masks to the device (GPU or CPU)\n",
    "        watermarked_imgs = watermarked_imgs.to(DEVICE).unsqueeze(0)  # Add batch dimension\n",
    "        masks = masks.to(DEVICE).unsqueeze(0)\n",
    "        clean_imgs = clean_imgs.to(DEVICE).unsqueeze(0)\n",
    "\n",
    "        # Zero out the gradients\n",
    "        part2_optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Predict cleaned image using the model\n",
    "        cleaned_img_pred = part2_model(watermarked_imgs, masks)\n",
    "\n",
    "        # Loss computation (MSE + Perceptual loss)\n",
    "        loss = part2_criterion(cleaned_img_pred, clean_imgs)\n",
    "\n",
    "        # Backpropagate the loss and update weights\n",
    "        loss.backward()\n",
    "        part2_optimizer.step()\n",
    "\n",
    "        # Accumulate loss for reporting\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{PART2_EPOCHS}, Loss: {epoch_loss / len(part2_train_loader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "PART2_MODEL_PATH = \"watermark_removal_model.pth\"\n",
    "torch.save(part2_model.state_dict(), PART2_MODEL_PATH)\n",
    "print(\"Watermark removal model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating images for the test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2477/2477 [04:36<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generation complete. Results saved in the output directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "part2_model.eval()\n",
    "\n",
    "os.makedirs(PART2_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Generating images for the test dataset...\")\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients for inference\n",
    "    for batch in tqdm(part2_test_loader, desc=\"Testing\"):\n",
    "        watermarked_imgs, masks, clean_imgs, filenames = batch[0]\n",
    "        watermarked_imgs = watermarked_imgs.to(DEVICE).unsqueeze(0)  # Add batch dimension\n",
    "        masks = masks.to(DEVICE).unsqueeze(0)\n",
    "\n",
    "        # Generate the clean image prediction\n",
    "        generated_img = part2_model(watermarked_imgs, masks)\n",
    "\n",
    "        # Convert the output tensor to a PIL image\n",
    "        generated_img = generated_img.squeeze(0).cpu().detach().clamp(0, 1)  # Remove batch dimension and clamp the values to [0, 1]\n",
    "        generated_img = transforms.ToPILImage()(generated_img)  # Convert to PIL image\n",
    "\n",
    "        # Ensure that the filename has an extension (e.g., .png)\n",
    "        output_filename = filenames[0]\n",
    "        if not output_filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            output_filename += '.png'  # Add a default extension if none exists\n",
    "\n",
    "        # Save the generated image\n",
    "        output_filepath = os.path.join(PART2_OUTPUT_DIR, f\"generated{output_filename}\")\n",
    "        generated_img.save(output_filepath)\n",
    "\n",
    "print(\"Image generation complete. Results saved in the output directory.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
